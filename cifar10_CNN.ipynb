{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ilayda/Desktop/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# File Path:\n",
    "CIFAR_DIR = ''\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    file = './cifar-10-batches-py/' + file\n",
    "    with open(file, 'rb') as fo:\n",
    "        cifar_dict = pickle.load(fo, encoding='bytes')\n",
    "    return cifar_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dirs = ['batches.meta','data_batch_1','data_batch_2','data_batch_3','data_batch_4','data_batch_5','test_batch']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = [0,1,2,3,4,5,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i,direc in zip(all_data,dirs):\n",
    "    all_data[i] = unpickle(CIFAR_DIR+direc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_meta = all_data[0]\n",
    "data_batch1 = all_data[1]\n",
    "data_batch2 = all_data[2]\n",
    "data_batch3 = all_data[3]\n",
    "data_batch4 = all_data[4]\n",
    "data_batch5 = all_data[5]\n",
    "test_batch = all_data[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'label_names': [b'airplane',\n",
       "  b'automobile',\n",
       "  b'bird',\n",
       "  b'cat',\n",
       "  b'deer',\n",
       "  b'dog',\n",
       "  b'frog',\n",
       "  b'horse',\n",
       "  b'ship',\n",
       "  b'truck'],\n",
       " b'num_cases_per_batch': 10000,\n",
       " b'num_vis': 3072}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([b'batch_label', b'labels', b'data', b'filenames'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_batch1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = data_batch1[b'data'].reshape(10000, 3, 32, 32).transpose(0, 2, 3, 1).astype('uint8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_encode(vec, vals=10):\n",
    "    n = len(vec)\n",
    "    out = np.zeros((n, vals))\n",
    "    out[range(n), vec] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CifarHelper():\n",
    "    def __init__(self):\n",
    "        self.i = 0\n",
    "        # Grabbing all the data batches\n",
    "        self.all_train_batches = [data_batch1, data_batch2, data_batch3, data_batch4, data_batch5]\n",
    "        self.test_batch = [test_batch]\n",
    "        # Initializing empty variables\n",
    "        self.training_images = None\n",
    "        self.training_labels = None\n",
    "        self.test_images = None\n",
    "        self.test_labels = None\n",
    "    def set_up_images(self):\n",
    "        print('Setting up training images and labels')\n",
    "        # Stacking training images vertically\n",
    "        self.training_images = np.vstack([d[b'data'] for d in self.all_train_batches])\n",
    "        train_len = len(self.training_images)\n",
    "        # Reshaping and normalizing of training images\n",
    "        self.training_images = self.training_images.reshape(train_len, 3, 32, 32).transpose(0, 2, 3, 1)/255\n",
    "        # Encoding the training lables\n",
    "        self.training_labels = one_hot_encode(np.hstack([d[b'labels'] for d in self.all_train_batches]), 10)\n",
    "        print('Setting up test images and labels')\n",
    "        # Stacking test images vertically\n",
    "        self.test_images = np.vstack([d[b'data'] for d in self.test_batch])\n",
    "        test_len = len(self.test_images)\n",
    "        # Reshaping and normalizing of training images\n",
    "        self.test_images = self.test_images.reshape(test_len, 3, 32, 32).transpose(0, 2, 3, 1)/255\n",
    "        # Encoding the training lables\n",
    "        self.test_labels = one_hot_encode(np.hstack([d[b'labels'] for d in self.test_batch]), 10)\n",
    "    def next_batch(self, batch_size):\n",
    "        x = self.training_images[self.i:self.i+batch_size].reshape(100, 32, 32, 3)\n",
    "        y = self.training_labels[self.i:self.i+batch_size]\n",
    "        self.i = (self.i + batch_size) % len(self.training_images)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "y_true = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "hold_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_weights(shape):\n",
    "    init_random_dist = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(init_random_dist)\n",
    "\n",
    "def init_bias(shape):\n",
    "    init_bias_vals = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(init_bias_vals)\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2by2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "def convolutional_layer(input_x, shape):\n",
    "    W = init_weights(shape)\n",
    "    b = init_bias([shape[3]])\n",
    "    return tf.nn.relu(conv2d(input_x, W)+b)\n",
    "\n",
    "def normal_full_layer(input_layer, size):\n",
    "    input_size = int(input_layer.get_shape()[1])\n",
    "    W = init_weights([input_size, size])\n",
    "    b = init_bias([size])\n",
    "    return tf.matmul(input_layer, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convo_1 = convolutional_layer(x, shape=[4, 4, 3, 32])\n",
    "convo_1_pooling = max_pool_2by2(convo_1)\n",
    "\n",
    "convo_2 = convolutional_layer(convo_1_pooling, shape=[4, 4, 32, 64])\n",
    "convo_2_pooling = max_pool_2by2(convo_2)\n",
    "\n",
    "convo_2_flat = tf.reshape(convo_2_pooling, [-1, 8*8*64])\n",
    "full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat, 1024))\n",
    "\n",
    "\n",
    "\n",
    "full_one_dropout = tf.nn.dropout(full_layer_one, keep_prob=hold_prob)\n",
    "\n",
    "y_pred = normal_full_layer(full_one_dropout, 10)\n",
    "\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=y_pred))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "train = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training images and labels\n",
      "Setting up test images and labels\n"
     ]
    }
   ],
   "source": [
    "ch = CifarHelper()\n",
    "ch.set_up_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0\n",
      "Accuracy:\n",
      "0.0911\n",
      "\n",
      "\n",
      "Step 100\n",
      "Accuracy:\n",
      "0.4146\n",
      "\n",
      "\n",
      "Step 200\n",
      "Accuracy:\n",
      "0.4779\n",
      "\n",
      "\n",
      "Step 300\n",
      "Accuracy:\n",
      "0.4832\n",
      "\n",
      "\n",
      "Step 400\n",
      "Accuracy:\n",
      "0.5097\n",
      "\n",
      "\n",
      "Step 500\n",
      "Accuracy:\n",
      "0.5423\n",
      "\n",
      "\n",
      "Step 600\n",
      "Accuracy:\n",
      "0.5555\n",
      "\n",
      "\n",
      "Step 700\n",
      "Accuracy:\n",
      "0.5764\n",
      "\n",
      "\n",
      "Step 800\n",
      "Accuracy:\n",
      "0.5813\n",
      "\n",
      "\n",
      "Step 900\n",
      "Accuracy:\n",
      "0.5847\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(steps):\n",
    "        batch = ch.next_batch(100)\n",
    "        sess.run(train, feed_dict={x: batch[0], y_true: batch[1], hold_prob: 0.5})\n",
    "        if i%100 == 0:\n",
    "            print('Step {}'.format(i))\n",
    "            print('Accuracy:')\n",
    "            matches = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y_true, 1))\n",
    "            acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
    "            print(sess.run(acc, feed_dict={x: ch.test_images, y_true: ch.test_labels, hold_prob: 1.0}))\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
